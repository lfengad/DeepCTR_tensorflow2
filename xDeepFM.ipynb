{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import *\n",
    "import tensorflow.keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import *\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# from tensorflow.keras.constraints import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/disk/share/criteo/'\n",
    "data = pd.read_csv(path+'criteo_sampled_data.csv')\n",
    "cols = data.columns.values\n",
    "\n",
    "dense_feats = [f for f in cols if f[0] == \"I\"]\n",
    "sparse_feats = [f for f in cols if f[0] == \"C\"]\n",
    "\n",
    "def process_dense_feats(data, feats):\n",
    "    d = data.copy()\n",
    "    d = d[feats].fillna(0.0)\n",
    "    for f in feats:\n",
    "        d[f] = d[f].apply(lambda x: np.log(x+1) if x > -1 else -1)\n",
    "    \n",
    "    return d\n",
    "\n",
    "data_dense = process_dense_feats(data, dense_feats)\n",
    "\n",
    "vocab_sizes = {}\n",
    "def process_sparse_feats(data, feats):\n",
    "    d = data.copy()\n",
    "    d = d[feats].fillna(\"-1\")\n",
    "    for f in feats:\n",
    "        label_encoder = LabelEncoder()\n",
    "        d[f] = label_encoder.fit_transform(d[f])\n",
    "        vocab_sizes[f] = d[f].nunique() + 1\n",
    "    return d\n",
    "\n",
    "data_sparse = process_sparse_feats(data, sparse_feats)\n",
    "total_data = pd.concat([data_dense, data_sparse], axis=1)\n",
    "total_data['label'] = data['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自定义层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseEmbedding(Layer):\n",
    "    def __init__(self, sparse_feats, vocab_sizes, embed_dims=8):\n",
    "        super().__init__()\n",
    "        self.sparse_feats = sparse_feats\n",
    "        self.vocab_sizes = vocab_sizes\n",
    "        self.embed_dims = embed_dims\n",
    "        \n",
    "        # 离散特征嵌入矩阵\n",
    "        self.sparse_embeds_mat = []\n",
    "        for idx, feat in enumerate(self.sparse_feats):\n",
    "            # reg = tf.keras.regularizers.l2(0.5)\n",
    "            emb = Embedding(input_dim=self.vocab_sizes[feat],\n",
    "                            output_dim=self.embed_dims,\n",
    "                            # embeddings_regularizer=reg,\n",
    "                            name=f'{feat}_emb')\n",
    "            self.sparse_embeds_mat.append(emb)\n",
    "        \n",
    "    def call(self, sparse_inputs):\n",
    "        sparse_embeds = []\n",
    "        for idx, emb_mat in enumerate(self.sparse_embeds_mat):\n",
    "            emb = emb_mat(sparse_inputs[idx])\n",
    "            sparse_embeds.append(emb)\n",
    "        concat_sparse_embeds = Concatenate(axis=1)(sparse_embeds)\n",
    "        return concat_sparse_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    def __init__(self, sparse_feats, vocab_sizes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 离散特1d征嵌入矩阵\n",
    "        self.sparse_1d_embeds = SparseEmbedding(sparse_feats, vocab_sizes, embed_dims=1)\n",
    "        \n",
    "        self.fc_dense = Dense(1)\n",
    "        self.fc_sparse = Dense(1)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        dense_inputs, sparse_inputs = inputs[0], inputs[1]\n",
    "        # 线性部分\n",
    "        concat_dense_inputs = Concatenate(axis=1)(dense_inputs)\n",
    "        first_order_dense_layer = self.fc_dense(concat_dense_inputs)\n",
    "        \n",
    "        concat_sparse_embeds_1d = self.sparse_1d_embeds(sparse_inputs) \n",
    "        flat_sparse_embeds_1d = Flatten()(concat_sparse_embeds_1d)\n",
    "        first_order_sparse_layer = self.fc_sparse(flat_sparse_embeds_1d)\n",
    "        \n",
    "        linear_output = Add()([first_order_dense_layer, first_order_sparse_layer])\n",
    "        return linear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class cross_layer(Layer):\n",
    "    def __init__(self, n_filters):\n",
    "        super().__init__()\n",
    "        # self.n_filters = n_filters \n",
    "        self.con1d = Conv1D(filters=n_filters, kernel_size=1, strides=1)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x0, xl = inputs\n",
    "        h = xl.shape[1]\n",
    "        m = x0.shape[1]\n",
    "        D = x0.shape[-1] # emb_dim\n",
    "\n",
    "        xl = tf.expand_dims(xl, -2) \n",
    "        xl = tf.tile(xl, [1, 1, m, 1]) # ?, h, m, D\n",
    "        x0 = tf.expand_dims(x0, -3) \n",
    "        x0 = tf.tile(x0, [1, h, 1, 1]) # ?, h, m, D\n",
    "        feature_maps = tf.multiply(xl, x0) # ?, h, m, D\n",
    "\n",
    "        # ?, h*m, D\n",
    "        feature_maps = tf.reshape(feature_maps, [-1, h*m, D])\n",
    "        # ?, D, h*m\n",
    "        feature_maps = tf.transpose(feature_maps, [0,2,1])\n",
    "        # ?, D, n_filters\n",
    "        feature_maps = self.con1d(feature_maps)\n",
    "        # ?, n_filters, D\n",
    "        feature_maps = tf.transpose(feature_maps, [0,2,1])\n",
    "        return feature_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIN(Layer):\n",
    "    def __init__(self, n_layers, n_filters):\n",
    "        super().__init__()\n",
    "        self.cross_layers = []\n",
    "        for i in range(n_layers):\n",
    "            self.cross_layers.append(cross_layer(n_filters))\n",
    "    def call(self, inputs):\n",
    "        x0 = xl = inputs\n",
    "        sum_poolings = []\n",
    "        for layer in self.cross_layers:\n",
    "            # ?, n_filters, D\n",
    "            xl = layer([x0, xl])\n",
    "            # ?, n_filters\n",
    "            sum_poolings.append(tf.reduce_sum(xl, axis=-1))\n",
    "            \n",
    "        return tf.concat(sum_poolings, axis=-1) # ?, n_filters*n_layers\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(Layer):\n",
    "    def __init__(self, hid_units=[256,256,256], use_dropout=True, output_unit=16):\n",
    "        super().__init__()\n",
    "        self.hid_units = hid_units\n",
    "        self.use_dropout = use_dropout\n",
    "        self.output_unit = output_unit\n",
    "        self.Dropout = Dropout(0.3)\n",
    "        self.dense_layers = []\n",
    "        for unit in self.hid_units:\n",
    "            self.dense_layers.append(Dense(unit, activation='relu'))\n",
    "        self.dense_layers.append(Dense(self.output_unit))\n",
    "        \n",
    "    def call(self, concat_sparse_embeds):\n",
    "        flat_sparse_embed = Flatten()(concat_sparse_embeds)\n",
    "        \n",
    "        x = self.dense_layers[0](flat_sparse_embed)\n",
    "        for dense in self.dense_layers[1:]:\n",
    "            x = dense(x)\n",
    "            if self.use_dropout:\n",
    "                x = self.Dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建模型 (keras函数式)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class xDeepFM:\n",
    "    def __init__(self, dense_feats, sparse_feats, vocab_sizes, \n",
    "                 embed_dims=8, cross_layer_num=3):\n",
    "        \n",
    "        # 连续特征\n",
    "        self.dense_inputs = []\n",
    "        for feat in dense_feats:\n",
    "            self.dense_inputs.append(Input(shape=1, name=feat))\n",
    "            \n",
    "        # 离散特征\n",
    "        self.sparse_inputs = []\n",
    "        for feat in sparse_feats:\n",
    "            self.sparse_inputs.append(Input(shape=1, name=feat))\n",
    "        \n",
    "        self.SparseEmbedding = SparseEmbedding(sparse_feats, vocab_sizes, embed_dims=8)\n",
    "        \n",
    "        self.linear = Linear(sparse_feats, vocab_sizes)\n",
    "        \n",
    "        self.CIN = CIN(n_layers=3, n_filters=6)\n",
    "        \n",
    "        self.DNN = DNN()\n",
    "        self.dense = Dense(1, activation='sigmoid')\n",
    "        \n",
    "    def bulid_model(self):\n",
    "        all_inputs = [self.dense_inputs, self.sparse_inputs]\n",
    "        linear_output = self.linear(all_inputs)\n",
    "        \n",
    "        # concat_dense_inputs = Concatenate(axis=1)(self.dense_inputs)\n",
    "        \n",
    "        concat_sparse_embeds = self.SparseEmbedding(self.sparse_inputs)\n",
    "        # flatten_sparse_embeds = Flatten()(concat_sparse_embeds)\n",
    "        \n",
    "        # concat_inputs = Concatenate(axis=1)([flatten_sparse_embeds, concat_dense_inputs])\n",
    "        cross_output = self.CIN(concat_sparse_embeds)\n",
    "        \n",
    "        fc_layer_output = self.DNN(concat_sparse_embeds)\n",
    "        \n",
    "        # 输出部分\n",
    "        concat_layer = Concatenate()([cross_output, fc_layer_output])\n",
    "        output = self.dense(concat_layer)\n",
    "        \n",
    "        model = Model(inputs=all_inputs, outputs=output)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "1954/1954 [==============================] - 35s 18ms/step - loss: 0.5071 - binary_crossentropy: 0.5071 - auc: 0.7214 - val_loss: 0.5005 - val_binary_crossentropy: 0.5005 - val_auc: 0.7355 - lr: 0.0010\n",
      "Epoch 2/3\n",
      " 914/1954 [=============>................] - ETA: 17s - loss: 0.4825 - binary_crossentropy: 0.4825 - auc: 0.7582"
     ]
    },
    {
     "ename": "_NotOkStatusException",
     "evalue": "InvalidArgumentError: Error while reading CompositeTensor._type_spec.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_NotOkStatusException\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-ff0fbfa3b92d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m model.fit([train_dense_x_all, train_sparse_x_all], train_label_all, batch_size=256,\n\u001b[1;32m     32\u001b[0m          \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_dense_x_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_sparse_x_all\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_label_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m          callbacks=callbacks, epochs=3)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2417\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2419\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2420\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2735\u001b[0m           *args, **kwargs)\n\u001b[1;32m   2736\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2737\u001b[0;31m     \u001b[0mcache_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2739\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_cache_key\u001b[0;34m(self, args, kwargs, include_tensor_ranks_only)\u001b[0m\n\u001b[1;32m   2573\u001b[0m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2574\u001b[0m       input_signature = pywrap_tfe.TFE_Py_EncodeArg(inputs,\n\u001b[0;32m-> 2575\u001b[0;31m                                                     include_tensor_ranks_only)\n\u001b[0m\u001b[1;32m   2576\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2577\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31m_NotOkStatusException\u001b[0m: InvalidArgumentError: Error while reading CompositeTensor._type_spec."
     ]
    }
   ],
   "source": [
    "train_data = total_data.loc[:500000-1]\n",
    "valid_data = total_data.loc[500000:]\n",
    "\n",
    "train_dense_x_all = [train_data[f].values for f in dense_feats]\n",
    "train_sparse_x_all = [train_data[f].values for f in sparse_feats]\n",
    "train_label_all = train_data[['label']].values\n",
    "\n",
    "val_dense_x_all = [valid_data[f].values for f in dense_feats]\n",
    "val_sparse_x_all = [valid_data[f].values for f in sparse_feats]\n",
    "val_label_all = valid_data[['label']].values\n",
    "\n",
    "\n",
    "model = xDeepFM(dense_feats, sparse_feats, vocab_sizes).bulid_model()\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', \n",
    "              metrics=['binary_crossentropy', 'AUC']) # tf.keras.metrics.AUC()\n",
    "\n",
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "checkpoints = ModelCheckpoint('checkpoints/model.h5', monitor='val_auc', \n",
    "                              mode='max', save_weights_only=True)# save_best_only=True\n",
    "early_stopping = EarlyStopping(monitor='val_auc', min_delta=0.0001, patience=2)\n",
    "def scheduler(epoch):\n",
    "    thred = 10\n",
    "    if epoch < thred:\n",
    "        return 0.001\n",
    "    else:\n",
    "        return 0.001 * tf.math.exp(0.1 * (thred - epoch))\n",
    "lr_schedule = LearningRateScheduler(scheduler)\n",
    "callbacks = [early_stopping, lr_schedule, checkpoints] # \n",
    "\n",
    "\n",
    "model.fit([train_dense_x_all, train_sparse_x_all], train_label_all, batch_size=256,\n",
    "         validation_data=([val_dense_x_all, val_sparse_x_all], val_label_all),\n",
    "         callbacks=callbacks, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 加载模型\n",
    "# model = DCN(dense_feats, sparse_feats, vocab_sizes).bulid_model()\n",
    "# model.load_weights('checkpoints/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
