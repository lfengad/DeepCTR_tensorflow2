{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import pickle as pkl\n",
    "import random\n",
    "import gzip\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import Model\n",
    "from layers import Dice\n",
    "from utils import DataIterator, prepare_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(Layer):\n",
    "    def __init__(self, user_count, item_count, cate_count, emb_dim, use_negsampling=False):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.use_negsampling = use_negsampling\n",
    "        self.user_emb = Embedding(user_count, self.emb_dim,\n",
    "                                  mask_zero=True, name=\"user_emb\")\n",
    "        self.item_emb = Embedding(item_count, self.emb_dim,\n",
    "                                  mask_zero=True, name=\"item_emb\")\n",
    "        self.cate_emb = Embedding(cate_count, self.emb_dim,\n",
    "                                  mask_zero=True, name=\"cate_emb\")\n",
    "        \n",
    "    def call(self, user, item, cate, item_his, cate_his,\n",
    "             noclick_item_his=[],  noclick_cate_his=[]):\n",
    "        user_emb = self.user_emb(user) # (B, D)\n",
    "        \n",
    "        # 基本属性embedding:\n",
    "        item_emb = self.item_emb(item) # (B, D)\n",
    "        cate_emb = self.cate_emb(cate) # (B, D)\n",
    "        item_join_emb = Concatenate(-1)([item_emb, cate_emb]) # (B, 2D)\n",
    "        \n",
    "        \n",
    "        # 历史行为序列embedding:\n",
    "        item_his_emb = self.item_emb(item_his) # (B, T, D)\n",
    "        cate_his_emb = self.item_emb(cate_his) # (B, T, D)\n",
    "        item_join_his_emb = Concatenate(-1)([item_his_emb, cate_his_emb]) # (B, T, 2D)\n",
    "        item_his_emb_sum = tf.reduce_sum(item_join_his_emb, axis=1) # (B, D)\n",
    "        \n",
    "        if self.use_negsampling:\n",
    "            # (B, T, neg_num, D)\n",
    "            noclick_item_his_emb = self.item_emb(noclick_item_his) \n",
    "            # (B, T, neg_num, D)\n",
    "            noclick_cate_his_emb = self.item_emb(noclick_cate_his) \n",
    "            # (B, T, neg_num, 2D)\n",
    "            noclick_item_join_his_emb = Concatenate(-1)([noclick_item_his_emb, noclick_cate_his_emb])\n",
    "            # (B, T, 2D)\n",
    "            noclick_item_emb_neg_sum = tf.reduce_sum(noclick_item_join_his_emb, axis=2) \n",
    "            # (B, 2D)\n",
    "            noclick_item_his_emb_sum = tf.reduce_sum(noclick_item_emb_neg_sum, axis=1) \n",
    "            # 只取出第一个负样本构成序列，(B, T, 2D)\n",
    "            noclick_item_join_his_emb = noclick_item_join_his_emb[:, :, 0, :] \n",
    "            # # (B, T, 2D)\n",
    "            # noclick_item_join_his_emb = tf.squeeze(noclick_item_join_his_emb, 2)\n",
    "            \n",
    "            return user_emb, item_join_emb, \\\n",
    "                    item_join_his_emb, item_his_emb_sum, \\\n",
    "                    noclick_item_join_his_emb, noclick_item_his_emb_sum \n",
    "            \n",
    "        return user_emb, item_join_emb, \\\n",
    "                item_join_his_emb, item_his_emb_sum\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCLayer(Layer):\n",
    "    def __init__(self, hid_dims=[80, 40, 2], use_dice=False):\n",
    "        super().__init__()\n",
    "        self.hid_dims = hid_dims\n",
    "        self.use_dice = use_dice\n",
    "        self.bn = BatchNormalization()\n",
    "        self.fc = []\n",
    "        self.dice = []\n",
    "        for dim in self.hid_dims[:-1]:\n",
    "            if use_dice:\n",
    "                self.fc.append(Dense(dim, name=f'dense_{dim}'))\n",
    "                self.dice.append(Dice())\n",
    "            else:\n",
    "                self.fc.append(Dense(dim, activation=\"sigmoid\", \n",
    "                                     name=f'dense_{dim}'))\n",
    "        self.fc.append(Dense(self.hid_dims[-1], name=\"dense_output\"))\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        inputs = self.bn(inputs)\n",
    "        if self.use_dice:\n",
    "            fc_out = inputs\n",
    "            for i in range(len(self.dice)):\n",
    "                fc_out = self.fc[i](fc_out)\n",
    "                fc_out = self.dice[i](fc_out)\n",
    "            fc_out = self.fc[-1](fc_out)\n",
    "            return fc_out\n",
    "        else: \n",
    "            fc_out = self.fc[0](inputs)\n",
    "            for fc in self.fc[1:]:\n",
    "                fc_out = fc(fc_out)\n",
    "            return fc_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算注意力得分\n",
    "class DINAttenLayer(Layer):\n",
    "    def __init__(self, hid_dims=[80, 40, 1]):\n",
    "        super().__init__()\n",
    "        self.FCLayer = FCLayer(hid_dims)\n",
    "        \n",
    "    def call(self, query, facts, mask):\n",
    "        \"\"\"\n",
    "        query: (B, 2D)\n",
    "        facts: (B, T, 2D)\n",
    "        mask: (B, T)\n",
    "        \"\"\"\n",
    "        mask = tf.equal(mask, tf.ones_like(mask)) # (B, T)\n",
    "        queries = tf.tile(query, [1, facts.shape[1]]) # (B, 2D*T)\n",
    "        queries = tf.reshape(queries, [-1, facts.shape[1], facts.shape[2]]) # # (B, T, 2D)\n",
    "        # (B, T, 2D*4)\n",
    "        din_all = tf.concat([queries, facts, queries - facts, queries * facts], axis=-1)\n",
    "        \n",
    "        fc_out = self.FCLayer(din_all) # (B, T, 1)\n",
    "        score = fc_out # (B, T, 1)\n",
    "        key_masks = tf.expand_dims(mask, 2) # (B, T) -> (B, T, 1)\n",
    "        padding = tf.ones_like(score) * (-2**32 + 1)\n",
    "        # True的地方为score，否则为极大的负数\n",
    "        score = tf.where(key_masks, score, padding) # (B, T, 1)\n",
    "        score = tf.nn.softmax(score) # (B, T, 1)\n",
    "        \n",
    "        return score\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuxTrainLayer(Layer):\n",
    "    def __init__(self, hid_dims=[100, 50, 1]):\n",
    "        super().__init__()\n",
    "        self.clk_fc = FCLayer(hid_dims)\n",
    "        self.noclk_fc = FCLayer(hid_dims)\n",
    "        \n",
    "    def call(self, h_states, click_seq, noclick_seq, mask):\n",
    "        mask = tf.cast(mask, tf.float32)\n",
    "        seq_len = click_seq.shape[1] # T-1\n",
    "        \n",
    "        clk_input = tf.concat([h_states, click_seq], -1) # (B, T-1, 2D*2)\n",
    "        clk_prob = tf.sigmoid(self.clk_fc(clk_input)) # (B, T-1, 1)\n",
    "        # (B, T-1)\n",
    "        clk_loss = - tf.reshape(tf.math.log(clk_prob), [-1, seq_len]) * mask \n",
    "        \n",
    "        noclk_input = tf.concat([h_states, noclick_seq], -1) # (B, T-1, 2D*2)\n",
    "        noclk_prob = tf.sigmoid(self.clk_fc(noclk_input)) # (B, T-1, 1)\n",
    "        # (B, T-1)\n",
    "        noclk_loss = - tf.reshape(tf.math.log(1.0 - noclk_prob), [-1, seq_len]) * mask\n",
    "        # 不指定axis，则计算全部数值的平均值\n",
    "        aux_loss = tf.reduce_mean(clk_loss + noclk_loss)\n",
    "        return aux_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AUGRUCell(Layer):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "        # 作为一个 RNN 的单元，必须有state_size属性\n",
    "        # state_size 表示每个时间步输出的维度\n",
    "        self.state_size = units\n",
    "    \n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        # 输入数据是一个tupe: (gru_output, atten_scores)\n",
    "        # 因此，t时刻输入的x_t的维度为：\n",
    "        dim_xt = input_shape[0][-1]\n",
    "        \n",
    "        # 重置门对t时刻输入数据x的权重参数：\n",
    "        self.W_R_x = tf.Variable(tf.random.normal(shape=[dim_xt, self.units]), name='W_R_x')\n",
    "        # 重置门对t时刻输入隐藏状态state的权重参数：\n",
    "        self.W_R_s = tf.Variable(tf.random.normal(shape=[self.units, self.units]), name='W_R_s')\n",
    "        # 重置门偏置项参数：\n",
    "        self.W_R_b = tf.Variable(tf.random.normal(shape=[self.units]), name='W_R_b')\n",
    "        \n",
    "        \n",
    "        # 更新门对t时刻输入数据x的权重参数：\n",
    "        self.W_U_x = tf.Variable(tf.random.normal(shape=[dim_xt, self.units]), name='W_U_x')\n",
    "        # 更新门对t时刻输入隐藏状态state的权重参数：\n",
    "        self.W_U_s = tf.Variable(tf.random.normal(shape=[self.units, self.units]), name='W_U_s')\n",
    "        # 更新门偏置项参数：\n",
    "        self.W_U_b = tf.Variable(tf.random.normal(shape=[self.units]), name='W_U_b')\n",
    "        \n",
    "        \n",
    "        # 候选隐藏状态 ~h_t 对t时刻输入数据x的权重参数：\n",
    "        self.W_H_x = tf.Variable(tf.random.normal(shape=[dim_xt, self.units]), name='W_H_x')\n",
    "        # 候选隐藏状态 ~h_t 对t时刻输入隐藏状态state的权重参数：\n",
    "        self.W_H_s = tf.Variable(tf.random.normal(shape=[self.units, self.units]), name='W_H_s')\n",
    "        # 候选隐藏状态 ~h_t 偏置项参数：\n",
    "        self.W_H_b = tf.Variable(tf.random.normal(shape=[self.units]), name='W_H_b')\n",
    "        \n",
    "    \n",
    "    def call(self, inputs, states):\n",
    "        x_t, att_score = inputs\n",
    "        states = states[0]\n",
    "        \"\"\"\n",
    "        x_t: x_(t), shape=(B, 2D)\n",
    "        states: hidden_state_(t-1), shape=(B, units)\n",
    "        att_score: attention_score_(t),  shape=(B, 1)\n",
    "        \"\"\"\n",
    "        # 重置门\n",
    "        r_t = tf.sigmoid(tf.matmul(x_t, self.W_R_x) + tf.matmul(states, self.W_R_s) + self.W_R_b)\n",
    "        # 更新门\n",
    "        u_t = tf.sigmoid(tf.matmul(x_t, self.W_U_x) + tf.matmul(states, self.W_U_s) + self.W_U_b)\n",
    "        # 带有注意力的更新门\n",
    "        a_u_t = tf.multiply(att_score, u_t)\n",
    "        # 候选隐藏状态\n",
    "        _h_t = tf.tanh(tf.matmul(x_t, self.W_H_x) + tf.matmul(tf.multiply(r_t, states), self.W_H_s) \n",
    "                       + self.W_H_b)\n",
    "        # 输出值\n",
    "        h_t = tf.multiply(1-a_u_t, states) + tf.multiply(a_u_t, _h_t)\n",
    "        # 对gru而言，当前时刻的output与传递给下一时刻的state相同\n",
    "        next_state = h_t\n",
    "        \n",
    "        \n",
    "        return h_t, next_state # 第一个表示output\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 得到历史行为的embedding表示\n",
    "class DIEN(Model):\n",
    "    def __init__(self, user_count, item_count, cate_count, EMBEDDING_DIM, \n",
    "                 HIS_LEN = 100, use_negsampling = True, hid_dims=[200, 80, 2]):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.rnn_dim = EMBEDDING_DIM*2\n",
    "        \n",
    "        self.EmbLayer = EmbeddingLayer(user_count, item_count, cate_count, \n",
    "                                       EMBEDDING_DIM, use_negsampling)\n",
    "        \n",
    "        self.GRU = GRU(self.rnn_dim, return_sequences=True)\n",
    "        self.AuxTrainLayer = AuxTrainLayer()\n",
    "        self.AttenLayer = DINAttenLayer()\n",
    "        # self.AUGRU = AUGRU(EMBEDDING_DIM*2, return_state=True)\n",
    "        self.AUGRU = RNN(AUGRUCell(self.rnn_dim))\n",
    "        self.FCLayer = FCLayer(hid_dims, use_dice=True)\n",
    "        \n",
    "        \n",
    "    def call(self, user, item, cate, item_his, cate_his, mask, no_m_his, no_c_his):\n",
    "        # 转 0, 1 为 True, False \n",
    "        mask_bool = tf.cast(mask, tf.bool)\n",
    "        # 得到embedding\n",
    "        embs = self.EmbLayer(user, item, cate, item_his, cate_his, no_m_his, no_c_his)\n",
    "        # (B, 2D) \n",
    "        user_emb, item_emb, his_emb, his_emb_sum, noclk_his_emb, noclk_his_emb_sum = embs\n",
    "        \n",
    "        \n",
    "        # 第一层 GRU\n",
    "        # tf2.2中的大坑：\n",
    "        # 官方文档中第二个参数为mask，\n",
    "        # 但是不指定参数名字mask=mask_bool的话，\n",
    "        # 则mask_bool会当成参数initial_state的值\n",
    "        gru_output = self.GRU(his_emb, mask=mask_bool) # (B, T, 2D)\n",
    "        # 辅助损失函数\n",
    "        aux_loss = self.AuxTrainLayer(gru_output[:, :-1, :], \n",
    "                                      his_emb[:, 1:, :],\n",
    "                                      noclk_his_emb[:, 1:, :],\n",
    "                                      mask[:, 1:]) # (B,)\n",
    "        \n",
    "        # 计算目标item与历史item的attention分数\n",
    "        atten_scores = self.AttenLayer(item_emb, gru_output, mask) # (B, T, 1)\n",
    "        \n",
    "        # AUGRU\n",
    "        behavior_emb = self.AUGRU((gru_output, atten_scores), mask=mask_bool) # (B, 2D) \n",
    "        \n",
    "        # 全连接层\n",
    "        inp = tf.concat([user_emb, item_emb, his_emb_sum, behavior_emb, \n",
    "                         noclk_his_emb_sum], axis=-1)\n",
    "        output = self.FCLayer(inp)\n",
    "        logit = tf.nn.softmax(output)\n",
    "        return output, logit, aux_loss\n",
    "    \n",
    "    def train(self, user, item, cate, item_his, cate_his, mask, no_m_his, no_c_his, target):\n",
    "        output, _, aux_loss = self.call(user, item, cate, item_his, cate_his, mask, no_m_his, no_c_his)\n",
    "        loss = tf.keras.losses.categorical_crossentropy(target, output, from_logits=False)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        return loss, aux_loss\n",
    "        \n",
    "    def predict(self, user, item, cate, item_his, cate_his, mask):\n",
    "        _, pred, _ = self.call(user, item, cate, item_his, cate_his, mask)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"data/\"\n",
    "train_file = base_path + \"local_train_splitByUser\"\n",
    "test_file = base_path + \"local_test_splitByUser\"\n",
    "uid_voc = base_path + \"uid_voc.pkl\"\n",
    "mid_voc = base_path + \"mid_voc.pkl\"\n",
    "cat_voc = base_path + \"cat_voc.pkl\"\n",
    "batch_size = 128\n",
    "maxlen = 100\n",
    "\n",
    "train_data = DataIterator(train_file, uid_voc, mid_voc, cat_voc, \n",
    "                          batch_size, maxlen, shuffle_each_epoch=False)\n",
    "\n",
    "n_uid, n_mid, n_cat = train_data.get_n() # 用户数，电影数，类别数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 loss 2.068039, aux loss 1.479113\n",
      "batch 10 loss 0.737447, aux loss 1.313956\n",
      "batch 20 loss 0.695979, aux loss 1.392124\n",
      "batch 30 loss 0.693791, aux loss 1.234638\n",
      "batch 40 loss 0.694052, aux loss 1.386228\n",
      "batch 50 loss 0.693868, aux loss 1.237043\n",
      "batch 60 loss 0.695745, aux loss 1.386409\n",
      "batch 70 loss 0.691482, aux loss 1.265594\n",
      "batch 80 loss 0.695219, aux loss 1.385818\n",
      "batch 90 loss 0.693376, aux loss 1.352869\n",
      "batch 100 loss 0.694469, aux loss 1.383801\n",
      "batch 110 loss 0.694890, aux loss 1.273229\n",
      "batch 120 loss 0.699449, aux loss 1.385453\n",
      "batch 130 loss 0.694843, aux loss 1.351792\n",
      "batch 140 loss 0.698159, aux loss 1.382274\n",
      "batch 150 loss 0.689563, aux loss 1.388619\n",
      "batch 160 loss 0.691783, aux loss 1.387143\n",
      "batch 170 loss 0.695458, aux loss 1.229023\n",
      "batch 180 loss 0.691120, aux loss 1.390311\n",
      "batch 190 loss 0.693030, aux loss 1.242190\n",
      "batch 200 loss 0.694106, aux loss 1.375872\n",
      "batch 210 loss 0.690383, aux loss 1.282734\n",
      "batch 220 loss 0.691290, aux loss 1.378154\n",
      "batch 230 loss 0.690240, aux loss 1.293406\n",
      "batch 240 loss 0.694030, aux loss 1.379894\n",
      "batch 250 loss 0.697263, aux loss 1.302448\n",
      "batch 260 loss 0.694523, aux loss 1.380266\n",
      "batch 270 loss 0.689354, aux loss 1.306656\n",
      "batch 280 loss 0.688328, aux loss 1.382248\n",
      "batch 290 loss 0.693213, aux loss 1.270545\n",
      "batch 300 loss 0.694994, aux loss 1.380146\n",
      "batch 310 loss 0.692642, aux loss 1.236873\n",
      "batch 320 loss 0.689740, aux loss 1.382414\n",
      "batch 330 loss 0.691055, aux loss 1.355132\n",
      "batch 340 loss 0.690439, aux loss 1.378826\n",
      "batch 350 loss 0.696317, aux loss 1.305768\n",
      "batch 360 loss 0.692655, aux loss 1.380104\n",
      "batch 370 loss 0.683240, aux loss 1.282599\n",
      "batch 380 loss 0.694989, aux loss 1.376244\n",
      "batch 390 loss 0.693351, aux loss 1.324468\n",
      "batch 400 loss 0.686526, aux loss 1.380373\n",
      "batch 410 loss 0.690881, aux loss 1.349163\n",
      "batch 420 loss 0.686478, aux loss 1.374557\n",
      "batch 430 loss 0.684616, aux loss 1.378559\n",
      "batch 440 loss 0.680370, aux loss 1.373135\n",
      "batch 450 loss 0.672189, aux loss 1.330817\n",
      "batch 460 loss 0.704011, aux loss 1.378112\n",
      "batch 470 loss 0.666249, aux loss 1.338828\n",
      "batch 480 loss 0.686237, aux loss 1.386753\n",
      "batch 490 loss 0.672048, aux loss 1.237627\n",
      "batch 500 loss 0.689416, aux loss 1.375439\n",
      "batch 510 loss 0.679942, aux loss 1.274050\n",
      "batch 520 loss 0.684325, aux loss 1.384659\n",
      "batch 530 loss 0.684855, aux loss 1.281043\n",
      "batch 540 loss 0.660100, aux loss 1.394137\n",
      "batch 550 loss 0.687806, aux loss 1.240736\n",
      "batch 560 loss 0.690218, aux loss 1.380075\n",
      "batch 570 loss 0.673716, aux loss 1.320786\n",
      "batch 580 loss 0.693648, aux loss 1.368184\n",
      "batch 590 loss 0.696522, aux loss 1.289395\n",
      "batch 600 loss 0.679908, aux loss 1.387056\n",
      "batch 610 loss 0.709319, aux loss 1.291360\n",
      "batch 620 loss 0.686557, aux loss 1.369841\n",
      "batch 630 loss 0.674588, aux loss 1.263717\n",
      "batch 640 loss 0.692565, aux loss 1.350408\n",
      "batch 650 loss 0.683875, aux loss 1.221066\n",
      "batch 660 loss 0.688051, aux loss 1.356015\n",
      "batch 670 loss 0.674489, aux loss 1.261728\n",
      "batch 680 loss 0.682079, aux loss 1.373235\n",
      "batch 690 loss 0.678413, aux loss 1.279788\n",
      "batch 700 loss 0.696760, aux loss 1.372520\n",
      "batch 710 loss 0.681217, aux loss 1.269751\n",
      "batch 720 loss 0.677947, aux loss 1.359509\n",
      "batch 730 loss 0.668031, aux loss 1.331041\n",
      "batch 740 loss 0.666014, aux loss 1.342387\n",
      "batch 750 loss 0.682697, aux loss 1.269510\n",
      "batch 760 loss 0.656295, aux loss 1.341351\n",
      "batch 770 loss 0.707546, aux loss 1.251843\n",
      "batch 780 loss 0.674079, aux loss 1.345248\n",
      "batch 790 loss 0.657360, aux loss 1.243539\n",
      "batch 800 loss 0.652130, aux loss 1.358508\n",
      "batch 810 loss 0.663262, aux loss 1.269382\n",
      "batch 820 loss 0.660223, aux loss 1.349433\n",
      "batch 830 loss 0.662249, aux loss 1.220359\n",
      "batch 840 loss 0.682940, aux loss 1.371676\n",
      "batch 850 loss 0.671639, aux loss 1.279638\n",
      "batch 860 loss 0.690707, aux loss 1.366297\n",
      "batch 870 loss 0.664772, aux loss 1.311729\n",
      "batch 880 loss 0.653953, aux loss 1.337097\n",
      "batch 890 loss 0.645285, aux loss 1.337443\n",
      "batch 900 loss 0.689522, aux loss 1.330288\n",
      "batch 910 loss 0.658664, aux loss 1.241068\n",
      "batch 920 loss 0.677607, aux loss 1.334146\n",
      "batch 930 loss 0.654895, aux loss 1.277277\n",
      "batch 940 loss 0.667736, aux loss 1.347373\n",
      "batch 950 loss 0.640414, aux loss 1.171708\n",
      "batch 960 loss 0.687433, aux loss 1.355712\n",
      "batch 970 loss 0.661177, aux loss 1.277527\n",
      "batch 980 loss 0.672078, aux loss 1.361447\n",
      "batch 990 loss 0.664237, aux loss 1.194987\n",
      "batch 1000 loss 0.708602, aux loss 1.360277\n"
     ]
    }
   ],
   "source": [
    "model = DIEN(n_uid, n_mid, n_cat, 16)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-2)\n",
    "\n",
    "# 训练模型\n",
    "for i, (src, tgt) in enumerate(train_data):\n",
    "    data = prepare_data(src, tgt, maxlen=100, return_neg=True)\n",
    "    uids, mids, cats, mid_his, cat_his, mid_mask, target, sl, no_m_his, no_c_his = data\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss, aux_loss = model.train(uids, mids, cats, mid_his, cat_his, \n",
    "                                     mid_mask, no_m_his, no_c_his, target)\n",
    "        if i%10 == 0:\n",
    "            print(\"batch %d loss %f, aux loss %f\" % (i, loss.numpy(), aux_loss.numpy()))\n",
    "            \n",
    "        loss = loss + aux_loss\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, model.trainable_variables))\n",
    "    \n",
    "    if i == 1000:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
