{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import *\n",
    "import tensorflow.keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import *\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# from tensorflow.keras.constraints import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/disk/share/criteo/'\n",
    "data = pd.read_csv(path+'criteo_sampled_data.csv')\n",
    "cols = data.columns.values\n",
    "\n",
    "dense_feats = [f for f in cols if f[0] == \"I\"]\n",
    "sparse_feats = [f for f in cols if f[0] == \"C\"]\n",
    "\n",
    "def process_dense_feats(data, feats):\n",
    "    d = data.copy()\n",
    "    d = d[feats].fillna(0.0)\n",
    "    for f in feats:\n",
    "        d[f] = d[f].apply(lambda x: np.log(x+1) if x > -1 else -1)\n",
    "    \n",
    "    return d\n",
    "\n",
    "data_dense = process_dense_feats(data, dense_feats)\n",
    "\n",
    "vocab_sizes = {}\n",
    "def process_sparse_feats(data, feats):\n",
    "    d = data.copy()\n",
    "    d = d[feats].fillna(\"-1\")\n",
    "    for f in feats:\n",
    "        label_encoder = LabelEncoder()\n",
    "        d[f] = label_encoder.fit_transform(d[f])\n",
    "        vocab_sizes[f] = d[f].nunique() + 1\n",
    "    return d\n",
    "\n",
    "data_sparse = process_sparse_feats(data, sparse_feats)\n",
    "total_data = pd.concat([data_dense, data_sparse], axis=1)\n",
    "total_data['label'] = data['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自定义层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseEmbedding(Layer):\n",
    "    def __init__(self, sparse_feats, vocab_sizes, embed_dims=8):\n",
    "        super().__init__()\n",
    "        self.sparse_feats = sparse_feats\n",
    "        self.vocab_sizes = vocab_sizes\n",
    "        self.embed_dims = embed_dims\n",
    "        \n",
    "        # 离散特征嵌入矩阵\n",
    "        self.sparse_embeds_mat = []\n",
    "        for idx, feat in enumerate(self.sparse_feats):\n",
    "            # reg = tf.keras.regularizers.l2(0.5)\n",
    "            emb = Embedding(input_dim=self.vocab_sizes[feat],\n",
    "                            output_dim=self.embed_dims,\n",
    "                            # embeddings_regularizer=reg,\n",
    "                            name=f'{feat}_emb')\n",
    "            self.sparse_embeds_mat.append(emb)\n",
    "        \n",
    "    def call(self, sparse_inputs):\n",
    "        sparse_embeds = []\n",
    "        for idx, emb_mat in enumerate(self.sparse_embeds_mat):\n",
    "            emb = emb_mat(sparse_inputs[idx])\n",
    "            sparse_embeds.append(emb)\n",
    "        concat_sparse_embeds = Concatenate(axis=1)(sparse_embeds)\n",
    "        return concat_sparse_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class single_cross_layer(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        embed_dims = input_shape[0][-1]\n",
    "        self.w = self.add_weight(shape=[embed_dims, 1], name='w')\n",
    "        self.b = self.add_weight(shape=[embed_dims, 1], name='b')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x0, xl = inputs\n",
    "        x0 = tf.expand_dims(x0, -1)\n",
    "        xl = tf.expand_dims(xl, -1)\n",
    "        x0_xl = tf.matmul(x0, xl, transpose_b=True)\n",
    "        x_next = tf.matmul(x0_xl, self.w) + xl + self.b\n",
    "        x_next = tf.squeeze(x_next, axis=-1)\n",
    "        return x_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cross_layer(Layer):\n",
    "    def __init__(self, cross_layer_num):\n",
    "        super().__init__()\n",
    "        self.cross_layer_num = cross_layer_num\n",
    "        self.cross_layers = []\n",
    "        for i in range(cross_layer_num):\n",
    "            self.cross_layers.append(single_cross_layer())\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x0 = inputs\n",
    "        xl = self.cross_layers[0]([x0, x0])\n",
    "        for layer in self.cross_layers[1:]:\n",
    "            xl = layer([x0, xl])\n",
    "        return xl\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(Layer):\n",
    "    def __init__(self, hid_units=[256,256,256], use_dropout=True, output_unit=16):\n",
    "        super().__init__()\n",
    "        self.hid_units = hid_units\n",
    "        self.use_dropout = use_dropout\n",
    "        self.output_unit = output_unit\n",
    "        self.Dropout = Dropout(0.3)\n",
    "        self.dense_layers = []\n",
    "        for unit in self.hid_units:\n",
    "            self.dense_layers.append(Dense(unit, activation='relu'))\n",
    "        self.dense_layers.append(Dense(self.output_unit))\n",
    "        \n",
    "    def call(self, concat_sparse_embeds):\n",
    "        flat_sparse_embed = Flatten()(concat_sparse_embeds)\n",
    "        \n",
    "        x = self.dense_layers[0](flat_sparse_embed)\n",
    "        for dense in self.dense_layers[1:]:\n",
    "            x = dense(x)\n",
    "            if self.use_dropout:\n",
    "                x = self.Dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建模型 (keras函数式)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCN:\n",
    "    def __init__(self, dense_feats, sparse_feats, vocab_sizes, \n",
    "                 embed_dims=8, cross_layer_num=3):\n",
    "        \n",
    "        # 连续特征\n",
    "        self.dense_inputs = []\n",
    "        for feat in dense_feats:\n",
    "            self.dense_inputs.append(Input(shape=1, name=feat))\n",
    "            \n",
    "        # 离散特征\n",
    "        self.sparse_inputs = []\n",
    "        for feat in sparse_feats:\n",
    "            self.sparse_inputs.append(Input(shape=1, name=feat))\n",
    "        \n",
    "        self.SparseEmbedding = SparseEmbedding(sparse_feats, vocab_sizes, embed_dims=8)\n",
    "        \n",
    "        self.cross_layer = cross_layer(cross_layer_num)\n",
    "        \n",
    "        self.DNN = DNN()\n",
    "        self.dense = Dense(1, activation='sigmoid')\n",
    "        \n",
    "    def bulid_model(self):\n",
    "        all_inputs = [self.dense_inputs, self.sparse_inputs]\n",
    "        \n",
    "        concat_dense_inputs = Concatenate(axis=1)(self.dense_inputs)\n",
    "        \n",
    "        concat_sparse_embeds = self.SparseEmbedding(self.sparse_inputs)\n",
    "        flatten_sparse_embeds = Flatten()(concat_sparse_embeds)\n",
    "        \n",
    "        concat_inputs = Concatenate(axis=1)([flatten_sparse_embeds, concat_dense_inputs])\n",
    "        cross_output = self.cross_layer(concat_inputs)\n",
    "        \n",
    "        fc_layer_output = self.DNN(concat_sparse_embeds)\n",
    "        \n",
    "        # 输出部分\n",
    "        concat_layer = Concatenate()([cross_output, fc_layer_output])\n",
    "        output = self.dense(concat_layer)\n",
    "        \n",
    "        model = Model(inputs=all_inputs, outputs=output)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "1954/1954 [==============================] - 111s 57ms/step - loss: 0.4835 - binary_crossentropy: 0.4835 - auc: 0.7580 - val_loss: 0.4794 - val_binary_crossentropy: 0.4794 - val_auc: 0.7693 - lr: 0.0010\n",
      "Epoch 2/3\n",
      "1954/1954 [==============================] - 111s 57ms/step - loss: 0.4586 - binary_crossentropy: 0.4586 - auc: 0.7898 - val_loss: 0.4993 - val_binary_crossentropy: 0.4993 - val_auc: 0.7686 - lr: 0.0010\n",
      "Epoch 3/3\n",
      "1954/1954 [==============================] - 110s 56ms/step - loss: 0.4238 - binary_crossentropy: 0.4238 - auc: 0.8248 - val_loss: 0.5280 - val_binary_crossentropy: 0.5280 - val_auc: 0.7396 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f5d7f7d48d0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = total_data.loc[:500000-1]\n",
    "valid_data = total_data.loc[500000:]\n",
    "\n",
    "train_dense_x_all = [train_data[f].values for f in dense_feats]\n",
    "train_sparse_x_all = [train_data[f].values for f in sparse_feats]\n",
    "train_label_all = train_data[['label']].values\n",
    "\n",
    "val_dense_x_all = [valid_data[f].values for f in dense_feats]\n",
    "val_sparse_x_all = [valid_data[f].values for f in sparse_feats]\n",
    "val_label_all = valid_data[['label']].values\n",
    "\n",
    "\n",
    "model = DCN(dense_feats, sparse_feats, vocab_sizes).bulid_model()\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', \n",
    "              metrics=['binary_crossentropy', 'AUC']) # tf.keras.metrics.AUC()\n",
    "\n",
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "checkpoints = ModelCheckpoint('checkpoints/model.h5', monitor='val_auc', \n",
    "                              mode='max', save_weights_only=True)# save_best_only=True\n",
    "early_stopping = EarlyStopping(monitor='val_auc', min_delta=0.0001, patience=2)\n",
    "def scheduler(epoch):\n",
    "    thred = 10\n",
    "    if epoch < thred:\n",
    "        return 0.001\n",
    "    else:\n",
    "        return 0.001 * tf.math.exp(0.1 * (thred - epoch))\n",
    "lr_schedule = LearningRateScheduler(scheduler)\n",
    "callbacks = [early_stopping, lr_schedule, checkpoints] # \n",
    "\n",
    "\n",
    "model.fit([train_dense_x_all, train_sparse_x_all], train_label_all, batch_size=256,\n",
    "         validation_data=([val_dense_x_all, val_sparse_x_all], val_label_all),\n",
    "         callbacks=callbacks, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 加载模型\n",
    "# model = DCN(dense_feats, sparse_feats, vocab_sizes).bulid_model()\n",
    "# model.load_weights('checkpoints/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
